import Foundation

internal let ModelsData: [String: [String: Model]] = [
    "openai": [
        "gpt-4o-mini": Model(
            id: "gpt-4o-mini",
            name: "GPT-4o mini",
            api: .openAICompletions,
            provider: "openai",
            baseUrl: "https://api.openai.com/v1",
            reasoning: false,
            input: [.text, .image],
            cost: ModelCost(input: 0.15, output: 0.6, cacheRead: 0.08, cacheWrite: 0),
            contextWindow: 128000,
            maxTokens: 16384
        ),
        "gpt-5-mini": Model(
            id: "gpt-5-mini",
            name: "GPT-5 Mini",
            api: .openAIResponses,
            provider: "openai",
            baseUrl: "https://api.openai.com/v1",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 0.25, output: 2, cacheRead: 0.03, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
        "gpt-5": Model(
            id: "gpt-5",
            name: "GPT-5",
            api: .openAIResponses,
            provider: "openai",
            baseUrl: "https://api.openai.com/v1",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.25, output: 10, cacheRead: 0.13, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
        "gpt-5.2": Model(
            id: "gpt-5.2",
            name: "GPT-5.2",
            api: .openAIResponses,
            provider: "openai",
            baseUrl: "https://api.openai.com/v1",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.75, output: 14, cacheRead: 0.175, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
    ],
    "openai-codex": [
        "gpt-5.1": Model(
            id: "gpt-5.1",
            name: "GPT-5.1",
            api: .openAIResponses,
            provider: "openai-codex",
            baseUrl: "https://chatgpt.com/backend-api",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.25, output: 10, cacheRead: 0.125, cacheWrite: 0),
            contextWindow: 272000,
            maxTokens: 128000
        ),
        "gpt-5.1-codex-max": Model(
            id: "gpt-5.1-codex-max",
            name: "GPT-5.1 Codex Max",
            api: .openAIResponses,
            provider: "openai-codex",
            baseUrl: "https://chatgpt.com/backend-api",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.25, output: 10, cacheRead: 0.125, cacheWrite: 0),
            contextWindow: 272000,
            maxTokens: 128000
        ),
        "gpt-5.1-codex-mini": Model(
            id: "gpt-5.1-codex-mini",
            name: "GPT-5.1 Codex Mini",
            api: .openAIResponses,
            provider: "openai-codex",
            baseUrl: "https://chatgpt.com/backend-api",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 0.25, output: 2, cacheRead: 0.025, cacheWrite: 0),
            contextWindow: 272000,
            maxTokens: 128000
        ),
        "gpt-5.2": Model(
            id: "gpt-5.2",
            name: "GPT-5.2",
            api: .openAIResponses,
            provider: "openai-codex",
            baseUrl: "https://chatgpt.com/backend-api",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.75, output: 14, cacheRead: 0.175, cacheWrite: 0),
            contextWindow: 272000,
            maxTokens: 128000
        ),
        "gpt-5.2-codex": Model(
            id: "gpt-5.2-codex",
            name: "GPT-5.2 Codex",
            api: .openAIResponses,
            provider: "openai-codex",
            baseUrl: "https://chatgpt.com/backend-api",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.75, output: 14, cacheRead: 0.175, cacheWrite: 0),
            contextWindow: 272000,
            maxTokens: 128000
        ),
    ],
    "opencode": [
        "alpha-gd4": Model(
            id: "alpha-gd4",
            name: "Alpha GD4",
            api: .anthropicMessages,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0.5, output: 2, cacheRead: 0.15, cacheWrite: 0),
            contextWindow: 262144,
            maxTokens: 32768
        ),
        "alpha-glm-4.7": Model(
            id: "alpha-glm-4.7",
            name: "Alpha GLM-4.7",
            api: .openAICompletions,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0.6, output: 2.2, cacheRead: 0.6, cacheWrite: 0),
            contextWindow: 204800,
            maxTokens: 131072
        ),
        "big-pickle": Model(
            id: "big-pickle",
            name: "Big Pickle",
            api: .openAICompletions,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0, output: 0, cacheRead: 0, cacheWrite: 0),
            contextWindow: 200000,
            maxTokens: 128000
        ),
        "claude-3-5-haiku": Model(
            id: "claude-3-5-haiku",
            name: "Claude Haiku 3.5",
            api: .anthropicMessages,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen",
            reasoning: false,
            input: [.text, .image],
            cost: ModelCost(input: 0.8, output: 4, cacheRead: 0.08, cacheWrite: 1),
            contextWindow: 200000,
            maxTokens: 8192
        ),
        "claude-haiku-4-5": Model(
            id: "claude-haiku-4-5",
            name: "Claude Haiku 4.5",
            api: .anthropicMessages,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1, output: 5, cacheRead: 0.1, cacheWrite: 1.25),
            contextWindow: 200000,
            maxTokens: 64000
        ),
        "claude-opus-4-1": Model(
            id: "claude-opus-4-1",
            name: "Claude Opus 4.1",
            api: .anthropicMessages,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 15, output: 75, cacheRead: 1.5, cacheWrite: 18.75),
            contextWindow: 200000,
            maxTokens: 32000
        ),
        "claude-opus-4-5": Model(
            id: "claude-opus-4-5",
            name: "Claude Opus 4.5",
            api: .anthropicMessages,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 5, output: 25, cacheRead: 0.5, cacheWrite: 6.25),
            contextWindow: 200000,
            maxTokens: 64000
        ),
        "claude-sonnet-4": Model(
            id: "claude-sonnet-4",
            name: "Claude Sonnet 4",
            api: .anthropicMessages,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 3, output: 15, cacheRead: 0.3, cacheWrite: 3.75),
            contextWindow: 1000000,
            maxTokens: 64000
        ),
        "claude-sonnet-4-5": Model(
            id: "claude-sonnet-4-5",
            name: "Claude Sonnet 4.5",
            api: .anthropicMessages,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 3, output: 15, cacheRead: 0.3, cacheWrite: 3.75),
            contextWindow: 1000000,
            maxTokens: 64000
        ),
        "glm-4.6": Model(
            id: "glm-4.6",
            name: "GLM-4.6",
            api: .openAICompletions,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0.6, output: 2.2, cacheRead: 0.1, cacheWrite: 0),
            contextWindow: 204800,
            maxTokens: 131072
        ),
        "glm-4.7-free": Model(
            id: "glm-4.7-free",
            name: "GLM-4.7",
            api: .openAICompletions,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0, output: 0, cacheRead: 0, cacheWrite: 0),
            contextWindow: 204800,
            maxTokens: 131072
        ),
        "gpt-5": Model(
            id: "gpt-5",
            name: "GPT-5",
            api: .openAIResponses,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.07, output: 8.5, cacheRead: 0.107, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
        "gpt-5-codex": Model(
            id: "gpt-5-codex",
            name: "GPT-5 Codex",
            api: .openAIResponses,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.07, output: 8.5, cacheRead: 0.107, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
        "gpt-5-nano": Model(
            id: "gpt-5-nano",
            name: "GPT-5 Nano",
            api: .openAIResponses,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 0, output: 0, cacheRead: 0, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
        "gpt-5.1": Model(
            id: "gpt-5.1",
            name: "GPT-5.1",
            api: .openAIResponses,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.07, output: 8.5, cacheRead: 0.107, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
        "gpt-5.1-codex": Model(
            id: "gpt-5.1-codex",
            name: "GPT-5.1 Codex",
            api: .openAIResponses,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.07, output: 8.5, cacheRead: 0.107, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
        "gpt-5.1-codex-max": Model(
            id: "gpt-5.1-codex-max",
            name: "GPT-5.1 Codex Max",
            api: .openAIResponses,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.25, output: 10, cacheRead: 0.125, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
        "gpt-5.1-codex-mini": Model(
            id: "gpt-5.1-codex-mini",
            name: "GPT-5.1 Codex Mini",
            api: .openAIResponses,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 0.25, output: 2, cacheRead: 0.025, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
        "gpt-5.2": Model(
            id: "gpt-5.2",
            name: "GPT-5.2",
            api: .openAIResponses,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.75, output: 14, cacheRead: 0.175, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
        "grok-code": Model(
            id: "grok-code",
            name: "Grok Code Fast 1",
            api: .openAICompletions,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0, output: 0, cacheRead: 0, cacheWrite: 0),
            contextWindow: 256000,
            maxTokens: 256000
        ),
        "kimi-k2": Model(
            id: "kimi-k2",
            name: "Kimi K2",
            api: .openAICompletions,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: false,
            input: [.text],
            cost: ModelCost(input: 0.4, output: 2.5, cacheRead: 0.4, cacheWrite: 0),
            contextWindow: 262144,
            maxTokens: 262144
        ),
        "kimi-k2-thinking": Model(
            id: "kimi-k2-thinking",
            name: "Kimi K2 Thinking",
            api: .openAICompletions,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0.4, output: 2.5, cacheRead: 0.4, cacheWrite: 0),
            contextWindow: 262144,
            maxTokens: 262144
        ),
        "minimax-m2.1-free": Model(
            id: "minimax-m2.1-free",
            name: "MiniMax M2.1",
            api: .anthropicMessages,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0, output: 0, cacheRead: 0, cacheWrite: 0),
            contextWindow: 204800,
            maxTokens: 131072
        ),
        "qwen3-coder": Model(
            id: "qwen3-coder",
            name: "Qwen3 Coder",
            api: .openAICompletions,
            provider: "opencode",
            baseUrl: "https://opencode.ai/zen/v1",
            reasoning: false,
            input: [.text],
            cost: ModelCost(input: 0.45, output: 1.8, cacheRead: 0, cacheWrite: 0),
            contextWindow: 262144,
            maxTokens: 65536
        ),
    ],
    "minimax": [
        "MiniMax-M2": Model(
            id: "MiniMax-M2",
            name: "MiniMax-M2",
            api: .anthropicMessages,
            provider: "minimax",
            baseUrl: "https://api.minimax.io/anthropic",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0.3, output: 1.2, cacheRead: 0, cacheWrite: 0),
            contextWindow: 196608,
            maxTokens: 128000
        ),
        "MiniMax-M2.1": Model(
            id: "MiniMax-M2.1",
            name: "MiniMax-M2.1",
            api: .anthropicMessages,
            provider: "minimax",
            baseUrl: "https://api.minimax.io/anthropic",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0.3, output: 1.2, cacheRead: 0, cacheWrite: 0),
            contextWindow: 204800,
            maxTokens: 131072
        ),
    ],
    "vercel-ai-gateway": [
        "google/gemini-2.5-flash": Model(
            id: "google/gemini-2.5-flash",
            name: "Gemini 2.5 Flash",
            api: .anthropicMessages,
            provider: "vercel-ai-gateway",
            baseUrl: "https://ai-gateway.vercel.sh",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 0.3, output: 2.5, cacheRead: 0.03, cacheWrite: 0),
            contextWindow: 1000000,
            maxTokens: 64000
        ),
        "anthropic/claude-opus-4.5": Model(
            id: "anthropic/claude-opus-4.5",
            name: "Claude Opus 4.5",
            api: .anthropicMessages,
            provider: "vercel-ai-gateway",
            baseUrl: "https://ai-gateway.vercel.sh",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 5, output: 25, cacheRead: 0.5, cacheWrite: 6.25),
            contextWindow: 200000,
            maxTokens: 64000
        ),
        "openai/gpt-5.1-codex-max": Model(
            id: "openai/gpt-5.1-codex-max",
            name: "GPT 5.1 Codex Max",
            api: .anthropicMessages,
            provider: "vercel-ai-gateway",
            baseUrl: "https://ai-gateway.vercel.sh",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1.25, output: 10, cacheRead: 0.125, cacheWrite: 0),
            contextWindow: 400000,
            maxTokens: 128000
        ),
    ],
    "zai": [
        "glm-4.5": Model(
            id: "glm-4.5",
            name: "GLM-4.5",
            api: .openAICompletions,
            provider: "zai",
            baseUrl: "https://api.z.ai/api/coding/paas/v4",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0.6, output: 2.2, cacheRead: 0.11, cacheWrite: 0),
            contextWindow: 131072,
            maxTokens: 98304,
            compat: OpenAICompat(supportsDeveloperRole: false, thinkingFormat: .zai)
        ),
        "glm-4.5-air": Model(
            id: "glm-4.5-air",
            name: "GLM-4.5-Air",
            api: .openAICompletions,
            provider: "zai",
            baseUrl: "https://api.z.ai/api/coding/paas/v4",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0.2, output: 1.1, cacheRead: 0.03, cacheWrite: 0),
            contextWindow: 131072,
            maxTokens: 98304,
            compat: OpenAICompat(supportsDeveloperRole: false, thinkingFormat: .zai)
        ),
        "glm-4.5-flash": Model(
            id: "glm-4.5-flash",
            name: "GLM-4.5-Flash",
            api: .openAICompletions,
            provider: "zai",
            baseUrl: "https://api.z.ai/api/coding/paas/v4",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0, output: 0, cacheRead: 0, cacheWrite: 0),
            contextWindow: 131072,
            maxTokens: 98304,
            compat: OpenAICompat(supportsDeveloperRole: false, thinkingFormat: .zai)
        ),
        "glm-4.5v": Model(
            id: "glm-4.5v",
            name: "GLM-4.5V",
            api: .openAICompletions,
            provider: "zai",
            baseUrl: "https://api.z.ai/api/coding/paas/v4",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 0.6, output: 1.8, cacheRead: 0, cacheWrite: 0),
            contextWindow: 64000,
            maxTokens: 16384,
            compat: OpenAICompat(supportsDeveloperRole: false, thinkingFormat: .zai)
        ),
        "glm-4.6": Model(
            id: "glm-4.6",
            name: "GLM-4.6",
            api: .openAICompletions,
            provider: "zai",
            baseUrl: "https://api.z.ai/api/coding/paas/v4",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0.6, output: 2.2, cacheRead: 0.11, cacheWrite: 0),
            contextWindow: 204800,
            maxTokens: 131072,
            compat: OpenAICompat(supportsDeveloperRole: false, thinkingFormat: .zai)
        ),
        "glm-4.6v": Model(
            id: "glm-4.6v",
            name: "GLM-4.6V",
            api: .openAICompletions,
            provider: "zai",
            baseUrl: "https://api.z.ai/api/coding/paas/v4",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 0.3, output: 0.9, cacheRead: 0, cacheWrite: 0),
            contextWindow: 128000,
            maxTokens: 32768,
            compat: OpenAICompat(supportsDeveloperRole: false, thinkingFormat: .zai)
        ),
        "glm-4.7": Model(
            id: "glm-4.7",
            name: "GLM-4.7",
            api: .openAICompletions,
            provider: "zai",
            baseUrl: "https://api.z.ai/api/coding/paas/v4",
            reasoning: true,
            input: [.text],
            cost: ModelCost(input: 0.6, output: 2.2, cacheRead: 0.11, cacheWrite: 0),
            contextWindow: 204800,
            maxTokens: 131072,
            compat: OpenAICompat(supportsDeveloperRole: false, thinkingFormat: .zai)
        ),
    ],
    "amazon-bedrock": [
        "anthropic.claude-3-5-haiku-20241022-v1:0": Model(
            id: "anthropic.claude-3-5-haiku-20241022-v1:0",
            name: "Claude Haiku 3.5",
            api: .bedrockConverseStream,
            provider: "amazon-bedrock",
            baseUrl: "https://bedrock-runtime.us-east-1.amazonaws.com",
            reasoning: false,
            input: [.text, .image],
            cost: ModelCost(input: 0.8, output: 4, cacheRead: 0.08, cacheWrite: 1),
            contextWindow: 200000,
            maxTokens: 8192
        ),
        "anthropic.claude-3-5-sonnet-20240620-v1:0": Model(
            id: "anthropic.claude-3-5-sonnet-20240620-v1:0",
            name: "Claude Sonnet 3.5",
            api: .bedrockConverseStream,
            provider: "amazon-bedrock",
            baseUrl: "https://bedrock-runtime.us-east-1.amazonaws.com",
            reasoning: false,
            input: [.text, .image],
            cost: ModelCost(input: 3, output: 15, cacheRead: 0.3, cacheWrite: 3.75),
            contextWindow: 200000,
            maxTokens: 8192
        ),
        "anthropic.claude-3-5-sonnet-20241022-v2:0": Model(
            id: "anthropic.claude-3-5-sonnet-20241022-v2:0",
            name: "Claude Sonnet 3.5 v2",
            api: .bedrockConverseStream,
            provider: "amazon-bedrock",
            baseUrl: "https://bedrock-runtime.us-east-1.amazonaws.com",
            reasoning: false,
            input: [.text, .image],
            cost: ModelCost(input: 3, output: 15, cacheRead: 0.3, cacheWrite: 3.75),
            contextWindow: 200000,
            maxTokens: 8192
        ),
        "anthropic.claude-3-haiku-20240307-v1:0": Model(
            id: "anthropic.claude-3-haiku-20240307-v1:0",
            name: "Claude Haiku 3",
            api: .bedrockConverseStream,
            provider: "amazon-bedrock",
            baseUrl: "https://bedrock-runtime.us-east-1.amazonaws.com",
            reasoning: false,
            input: [.text, .image],
            cost: ModelCost(input: 0.25, output: 1.25, cacheRead: 0, cacheWrite: 0),
            contextWindow: 200000,
            maxTokens: 4096
        ),
        "anthropic.claude-3-opus-20240229-v1:0": Model(
            id: "anthropic.claude-3-opus-20240229-v1:0",
            name: "Claude Opus 3",
            api: .bedrockConverseStream,
            provider: "amazon-bedrock",
            baseUrl: "https://bedrock-runtime.us-east-1.amazonaws.com",
            reasoning: false,
            input: [.text, .image],
            cost: ModelCost(input: 15, output: 75, cacheRead: 0, cacheWrite: 0),
            contextWindow: 200000,
            maxTokens: 4096
        ),
        "anthropic.claude-3-sonnet-20240229-v1:0": Model(
            id: "anthropic.claude-3-sonnet-20240229-v1:0",
            name: "Claude Sonnet 3",
            api: .bedrockConverseStream,
            provider: "amazon-bedrock",
            baseUrl: "https://bedrock-runtime.us-east-1.amazonaws.com",
            reasoning: false,
            input: [.text, .image],
            cost: ModelCost(input: 3, output: 15, cacheRead: 0, cacheWrite: 0),
            contextWindow: 200000,
            maxTokens: 4096
        ),
        "global.anthropic.claude-haiku-4-5-20251001-v1:0": Model(
            id: "global.anthropic.claude-haiku-4-5-20251001-v1:0",
            name: "Claude Haiku 4.5",
            api: .bedrockConverseStream,
            provider: "amazon-bedrock",
            baseUrl: "https://bedrock-runtime.us-east-1.amazonaws.com",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 1, output: 5, cacheRead: 0.1, cacheWrite: 1.25),
            contextWindow: 200000,
            maxTokens: 64000
        ),
        "global.anthropic.claude-opus-4-5-20251101-v1:0": Model(
            id: "global.anthropic.claude-opus-4-5-20251101-v1:0",
            name: "Claude Opus 4.5",
            api: .bedrockConverseStream,
            provider: "amazon-bedrock",
            baseUrl: "https://bedrock-runtime.us-east-1.amazonaws.com",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 5, output: 25, cacheRead: 0.5, cacheWrite: 6.25),
            contextWindow: 200000,
            maxTokens: 64000
        ),
        "global.anthropic.claude-sonnet-4-20250514-v1:0": Model(
            id: "global.anthropic.claude-sonnet-4-20250514-v1:0",
            name: "Claude Sonnet 4",
            api: .bedrockConverseStream,
            provider: "amazon-bedrock",
            baseUrl: "https://bedrock-runtime.us-east-1.amazonaws.com",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 3, output: 15, cacheRead: 0.3, cacheWrite: 3.75),
            contextWindow: 200000,
            maxTokens: 64000
        ),
        "global.anthropic.claude-sonnet-4-5-20250929-v1:0": Model(
            id: "global.anthropic.claude-sonnet-4-5-20250929-v1:0",
            name: "Claude Sonnet 4.5",
            api: .bedrockConverseStream,
            provider: "amazon-bedrock",
            baseUrl: "https://bedrock-runtime.us-east-1.amazonaws.com",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 3, output: 15, cacheRead: 0.3, cacheWrite: 3.75),
            contextWindow: 200000,
            maxTokens: 64000
        ),
    ],
    "anthropic": [
        "claude-sonnet-4-5": Model(
            id: "claude-sonnet-4-5",
            name: "Claude Sonnet 4.5",
            api: .anthropicMessages,
            provider: "anthropic",
            baseUrl: "https://api.anthropic.com",
            reasoning: true,
            input: [.text, .image],
            cost: ModelCost(input: 3, output: 15, cacheRead: 0.3, cacheWrite: 3),
            contextWindow: 200000,
            maxTokens: 8192
        ),
        "claude-3-5-haiku-20241022": Model(
            id: "claude-3-5-haiku-20241022",
            name: "Claude Haiku 3.5",
            api: .anthropicMessages,
            provider: "anthropic",
            baseUrl: "https://api.anthropic.com",
            reasoning: false,
            input: [.text, .image],
            cost: ModelCost(input: 0.8, output: 4, cacheRead: 0.08, cacheWrite: 1),
            contextWindow: 200000,
            maxTokens: 8192
        ),
        "claude-3-5-haiku-latest": Model(
            id: "claude-3-5-haiku-latest",
            name: "Claude Haiku 3.5 (latest)",
            api: .anthropicMessages,
            provider: "anthropic",
            baseUrl: "https://api.anthropic.com",
            reasoning: false,
            input: [.text, .image],
            cost: ModelCost(input: 0.8, output: 4, cacheRead: 0.08, cacheWrite: 1),
            contextWindow: 200000,
            maxTokens: 8192
        )
    ]
]
